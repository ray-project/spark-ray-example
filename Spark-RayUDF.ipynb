{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Ray in PySpark\n",
    "\n",
    "Spark + AI Summit 2020 talk, \n",
    "[Dean Wampler](mailto:dean@anyscale.com)\n",
    "\n",
    "This notebook demonstrates one way to integrate Ray and PySpark applications, where Ray is embedded in a _UDF_. The use case simulates the requirement for _data governance_, where we want to trace each record processed by a PySpark job. \n",
    "\n",
    "Another, more conventional way to meet this requirement is to run a separate webservice and make remote calls to it (usually over HTTP). This approach is demonstrated in the `ray-serve` directory. (See the [README](README.md) for details.)\n",
    "\n",
    "This notebook embeds Ray in a UDF, where the Ray cluster is co-resident on the same nodes as PySpark. We'll actually just use a single machine, but the results generalize to real cluster deployments with minor changes (noted where applicable).\n",
    "\n",
    "Why use this approach instead of the standalone system? Here are the pros and cons:\n",
    "\n",
    "**Pros:**\n",
    "* Avoiding a network/HTTP call may be more efficient in many cases.\n",
    "* Fewer services to manage. Once PySpark and Ray clusters are setup, you can allow them to do all the scaling and distribution required. Spark handles the data partitions, Ray handles distribution of the other tasks and object graphs (for distributed state).\n",
    "\n",
    "**Cons:**\n",
    "* You might prefer explicitly separate services for runtime visibility and independent management. For example, it's easier to upgrade a separate web service behind a router, whereas in the example here the PySpark and Data Governance \"hook\" are more closely linked.\n",
    "\n",
    "You can learn more about Ray [here](http://ray.io).\n",
    "\n",
    "> **Note:** This notebook connects to a running Ray cluster. Start Ray ahead of time with `ray start --head`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** Requires Java 8!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!java -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, time\n",
    "import pyspark\n",
    "import ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DataType, BooleanType, NullType, IntegerType, StringType, MapType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a `DataGovernanceSystem` Ray actor that represents our governance system. (This is also defined in the file `data_governance_system.py`.) All it does is add each reported `id` to an internal collection. \n",
    "\n",
    "In a more realistic implementation, this class would be a \"hook\" that forwards the ids and other useful metadata asynchronously to a real governance system, like [Apache Atlas](http://atlas.apache.org/#/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "class DataGovernanceSystem:\n",
    "    def __init__(self, name = 'DataGovernanceSystem'):\n",
    "        self.name = name\n",
    "        self.ids = []\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    def log(self, id_to_log):\n",
    "        \"\"\"\n",
    "        Log record ids that have been processed.\n",
    "        Returns the new count.\n",
    "        \"\"\"\n",
    "        self.ids.append(id_to_log)\n",
    "        return self.get_count()\n",
    "\n",
    "    def get_ids(self):\n",
    "        \"\"\"Return the ids logged. Don't call this if the list is long!\"\"\"\n",
    "        return self.ids\n",
    "\n",
    "    def get_count(self):\n",
    "        \"\"\"Return the count of ids logged.\"\"\"\n",
    "        return len(self.ids)\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Forget all ids that have been logged.\"\"\"\n",
    "        self.ids = []\n",
    "\n",
    "    def get_start_time(self):\n",
    "        return self.start_time\n",
    "\n",
    "    def get_up_time(self):\n",
    "        return time.time() - self.start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a simple `Record` type with a `record_id` field, used for logging to `DataGovernanceSystem`, and an opaque `data` field with everything else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Record:\n",
    "    def __init__(self, record_id, data):\n",
    "        self.record_id = record_id\n",
    "        self.data = data\n",
    "    def __str__(self):\n",
    "        return f'Record(record_id={self.record_id},data={self.data})'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now initialize Ray in this application. Passing `address='auto'` tells Ray to connect to the running cluster. (If this node isn't part of that cluster, i.e., Ray isn't already running on this node, then pass the correct server address and port.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init(address='auto', ignore_reinit_error=True) # The `ignore_reinit_error=True` lets us rerun this cell without error..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Click here to open the Ray Dashboard: http://{ray.get_webui_url()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_name = 'dgs'\n",
    "gov = DataGovernanceSystem.options(name=actor_name, detached=True).remote()\n",
    "gov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then use it somewhere \"else\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dgs = ray.util.get_actor(actor_name)\n",
    "test_records = [Record(i, f'data: {i}') for i in range(3)] \n",
    "for record in test_records:\n",
    "    print(record)\n",
    "    dgs.log.remote(record.record_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gov_status():\n",
    "    dgs = ray.util.get_actor(actor_name)\n",
    "    print(f'count:   {ray.get(dgs.get_count.remote())}')\n",
    "    print(f'ids:     {ray.get(dgs.get_ids.remote())}')\n",
    "    print(f'up time: {ray.get(dgs.get_up_time.remote())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gov_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reset the server:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dgs.reset.remote()\n",
    "gov_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_record(id):\n",
    "    \"\"\"\n",
    "    This function will become a UDF for Spark. Since each Spark task runs in a separate process, \n",
    "    we'll initialize Ray, connecting to the running cluster, if it is not already initialized.\n",
    "    \"\"\"\n",
    "    did_initialization = 0\n",
    "    if not ray.is_initialized():\n",
    "        ray.init(address='auto', redis_password='5241590000000000')\n",
    "        did_initialization = 1\n",
    "        \n",
    "    dgs = ray.util.get_actor(actor_name)\n",
    "    count_id = dgs.log.remote(id)   # Runs asynchronously, returning an object id for a future.\n",
    "    count = ray.get(count_id)       # But this blocks!\n",
    "    return {'initialized': did_initialization, 'count': count}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = pyspark.sql.SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"Data Governance Example\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_record_udf = udf(lambda id: log_record(id), MapType(StringType(), IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_records=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = [Record(i, f'str: {i}') for i in range(num_records)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(records, ['id', 'data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ray = df.select('id', 'data', log_record_udf('id').alias('logged'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_ray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time df_ray.show(n=num_records, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the `logged` column, there are several PySpark processes (four on my laptop), each of which initializes Ray once.\n",
    "\n",
    "You probably also see that the `count` values are out of order, because updates happen asynchronously from several PySpark tasks to the single `DataGovernanceSystem` actor, but Ray's actor model handles thread-safe updates, so that the final count is correct! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gov_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gov.reset.remote()\n",
    "gov_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
